\chapter{Background}
%labels will help you to reference to certain images, tables, chapters, section, and so on...
\label{background}


\section{Speech Recognition}
Controlling a drone via voice commands will require several steps. First, \textit{Automatic Speech Recognition} (ASR) will need to be implemented. Since the drone will only be required to recognize and respond to a small set of words/phrases, creating a custom dictionary would be ideal (this will directly impact the memory size of the data set and can possibly result in more accurate interpretations). Next, the interpretations need to be processed and the drone's behavior needs to reflect the spoken dialog. Since the SEK is operating in dynamic and dangerous environments, this process needs to happen with high accuracy/precision and with as little delay as possible. 
\bigskip

ASR has two main approaches in implementation: either using \textit{Hidden Markov Models} (HMM) or \textit{Deep-Learning} (DL). Each implementation has a number of trade-offs and the manner of the implementation determines which approach is, in some sense, optimal. HMMs are typically easier to understand and implement (fewer parameters) at the cost of accuracy (\cite{Multimodal}). Creating HMMs for ASR requires five steps: Feature Extraction, an Acoustic Model, a Lexicon Model, a Language Model and then a Decoder (usually the Forward-Backward or Viterbi algorithms) (\cite{Stanford}). HMMs are good for problems that have a small number of states (grammar, words) and could allow for the drone to perform entirely offline (\cite{CMU1}). There is already a large library of existing solutions and implementations utilizing the CMUSphinx API (\cite{CMU-Sphinx}).
\smallskip

DL methods, usually with \textit{Convolutional Neural Networks} (CNN) or Recurrent Neural Networks (RNN) (or a combination CNN-RNN encoder/decoder \\ (\cite{Wang2016CNNRNNAU})), allow for higher accuracy in results given proper parameter tuning and a large enough dataset  (\cite{Song2015EndtoEndDN}).  DL implementations, on the other hand, will require that a network is trained with large amounts of data and the model be upload directly to the drone (or a microcontroller interface). In general, even using only an RNN will outperform an HMM (\cite{Connectionist}). More research needs to be completed prior to selecting the best candidate for ASR, as it currently seems that a DL method would be the better candidate due to its higher accuracy. The current goal is to achieve high-performance results (and an excellent demonstration). However, the focus will be to create/implement/simulate a speech recognition model that is useful for evaluating user cognitive load.
\smallskip

\textit{Text-to-Speech} and \textit{Speech-to-Text} are two methods of machine translation. Intuitively, these technologies are mappings to shift from one domain to the other. These are common in modern ASR technologies (like Alexa and Siri) for providing the user with a confirmation of audio commands. 

\section{Mixed Reality User Interface}
Once the drone can properly be given instructions via ASR, it is important that the drone is able to communicate back to the user in a meaningful manner. The main user-experience related criteria that are being pursued in this project is a 'seamless, natural integration of a drone into a mixed human-robot team.' This means that the drone needs to be able to communicate effectively (quickly, clearly, and without ambiguity). As such, there are very few approaches that would be sufficient in an environment that places a great deal of stress on the user. Haptic feedback (tactile vibrations) are likely not a good approach, as they can be easily misunderstood (and rely on the user being trained to interpret the message (\cite{Cho2003})). A speech response from the drone is also not ideal, as it requires that the user is primed for listening to the confirmation of the planned tasked (\cite{dysoncognitive}). During stress, the cognitive ability of the user will degrade (\cite{Broadbent1958-BROPAC-3}). This means that long response might be forgotten, misunderstood, or not properly heard (\cite{Engle}). Any of these events would require that the user ask the drone to repeat the flight plan. These are issues that exist even in human-human interactions, so it would be highly advantageous if a robotic system could mitigate or alleviate these issues. 
\smallskip

An alternative to tactile and audio feedback is visual feedback. This will improve the response time of the user and mitigate errors due to multi-modal bottlenecks in information processing (\cite{Sommer2001MultipleBI}). One approach would be to put LEDs on the drone that signal certain sequences of commands. Again, in this case, the operator needs to be trained to interpret these responses from the drone. Furthermore, once the drone is out of the line-of-sight, the drone will not be able to effectively communicate with visual commands. Therefore, a UI within a Mixed Reality device (i.e., HoloLens) can also be utilized. The UI can be designed in a mixed-reality format that displays, with written words or intuitive icons, the flight/task plan of the drone. It is hypothesized that this will mitigate the forgetfulness of the user due to the high cognitive load. The UI could be designed in a manner that will not impede the vision of the user. This allows the user to maintain situational awareness and analyze the feedback when deemed safe. Regardless, both audio and visual solutions need to be approached as often times theory and reality do not match in real-world implementations. 

%%\subsection{Gesture Recognition and Integration}
%To approach RQ3, the previous work taken place at the Distribution Artificial Intelligence Laboratory (DAI-Labor) would be extended \cite{Montebaur}. The general idea is to allow the drone to allow for multi-modal inputs (speech and/or visual). The motivation for this is that the more critical missions (espionage, infiltration) require users to be silent (covert operations). Therefore, speech-related commands need to be minimized. However, the noise the drone emits is still an issue. Therefore, it might be beneficial to allow the drone to receive flight/task plans prior to motor activation. 
\smallskip

%Furthermore, it might be the case that only small, manual movements are desired for the drone. A use-case scenario for this would be as follows: a drone had completed a flight plan to enter into a specific room but has not located any targets and is idling on location. The operator, using a mixed-reality or video-streaming device, has access to the feed of the drone. The human operator believes that he sees something that the drone does not and wants to make small adjustments to the drone's location or heading. This could be done with manual operation using the PeriSense controller. (Perhaps the user wants to view specific infrastructure damage, find a target hiding from the drone, or analyze other environmental hazards that the drone has not yet been taught how to observe). 

\section{Robot Behavior Planning}

In mixed human-robot teams, predictable and reliable agent behavior is crucial.  \cite{hrabia1} have created a framework that allows for multi-agent environments with adjustable parameters such as pose, distance, and movements. When considering having a drone (or another robot agent) adjust behavior depending on both the user and the environment (both dynamic), a flexible approach to decision making and planning is desired. This would allow for the drone to change is behavior in real-time without compromising the mobility/visibility of the operator or negatively interacting with the environment (i.e., crashing into a wall). 
\smallskip

The \textit{ROS Hybrid Behavior Planner} (RHBP) seeks to meet these requirements with a hybrid reactive-adaptive behavior-based planner. This is a major improvement over traditional approaches (conditional statements) that do not have any weights, probabilistic characteristics or allow for (potential) reinforcement learning models. 

\section{Cognitive Load Theory and Testing}
Many different tests have been created to evaluate the cognitive performance of individuals during various conditions. \textit{Situational Awareness} is limited by available resources in memory capacity and computation during decision-making (\cite{Endsley}). When resources are limited or have been depleted, users begin to make errors in accuracy or decision making as a result. Cognitive load (and SA) can be evaluated with two types of tests: qualitative and quantitative. A common qualitative test is the NASA Task Load Index (NASA-TXL) which allows users to access their \textit{perceived} stress and workload levels while completing other tasks. Another traditional test is based on Cognitive Load Theory (CLT) in which subjects report their perceived performance after tasks are given for problem-solving or spit-attention (\cite{Chandler}).


For qualitative experiments, attention, working memory, and response time are common metrics. The \textit{Operation Span Task} (OSPAN) is used to measure the \textit{Working Memory} capacity of users (\cite{ospan2}). This is completed by giving the user an item to remember and then interjecting with random simple mathematics problems, after which they are asked to recall the item. In the case of drone operations, a set of directions could be provided as the `item.' The \textit{Attentional Blink} test is used to measure the response time and accuracy of a user when two items are displayed for brief periods of time with a short (variable) delay between objects (\cite{Nieuwenstein}). NASA has employed the \textit{Psychomotor Vigilance Task} to measure the cognitive performance of astronauts during space missions (NASA Extreme Environment Mission Operation NEEMO (\cite{nasa2})) and aboard the International Space Station. This test measures the response time for the user to press a keyboard after an image has been displayed on a screen (faster is better). 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%DELETEME: This chapter will cover all of your background information and related work. Background and related work are directly related to your thesis. Please do not place irrelevant content here which is a common mistake. Citing will be handled in the appendices.

%DELETEME: Background represents underlaying knowledge that is required to understand your work. The expected knowledge level of your readers can be set to the one of a bachelor or master student who just finished his studies (depending on what kind of thesis you are writing). This means that you do not need to describe how computers work, unless your thesis topic is about this. Everything that an avarage alumni from your field of studies should now does not need to be described. It turn, background information that is very complex and content-wise very near to you problem, can be placed in the main parts. Everyting else should be written here. Note: it is important to connect each presented topic to your thesis. E.g. if you present the ISO/OSI layer model you should also write that this is needed to understand the protocols you plan to develop in the main parts.

%DELETEME: Related work respresents results from work that handled the same or a similar problem that you are addressing. This work might have used a different approach or might not have been that successful. Finding a paper / work that solved your problem in the same way you were planning to do is not good and you should contact your supervizor for solving this issue. Again, each paper / work has to be connected to your approach: other papers might have not chosen an optimal solution; they might not have been taking care of essential aspects; they might have chosen a different approach and you believe, yours will work better ...

%###################################################################################
%###################### Topic A             ########################################
%###################################################################################
%\section{Topic 1}

%###################################################################################
%###################### Topic B             ########################################
%###################################################################################
%\section{Topic 2}

%###################################################################################
%###################### Topic C             ########################################
%###################################################################################
%\section{Topic 3}